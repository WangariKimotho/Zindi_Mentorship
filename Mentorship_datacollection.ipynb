{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mentorship_datacollection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPKYoYzZCkROV8ecSHiNn4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WangariKimotho/Zindi_Mentorship/blob/main/Mentorship_datacollection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2yETMsrSiLf"
      },
      "source": [
        "##NOTEBOOK ON DATA COLLECTION FROM TWITTER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18ir5Sgfq33v"
      },
      "source": [
        "![Twitter](https://www.itsecurityguru.org/wp-content/uploads/2021/03/alexander-shatov-k1xf2D7jWUs-unsplash.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQBFPMPXStWA"
      },
      "source": [
        "This is a task carried out on my first week under the Zindi Mentorship program 2021. The [task](https://zindi.africa/competitions/afd-gender-based-violence-dataset-collection-challenge) is to collect data with information on gender based violence in Africa which has been coined the shadow pandemic affecting women and girls all across Africa.\n",
        "\n",
        "\n",
        "\n",
        "This is simply a demo on how to go about such a task and with that might not include everything that can be done when collecting data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32H1qs4eTXJ6"
      },
      "source": [
        "####**STEP1:Authorization from twitter**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-icrKyjThw0"
      },
      "source": [
        "We need to acquire access from the twitter admin and developer team to collect tweets.This is done first by having a twitter account(if you don't have one) then apply for a developer account to access the API here: https://developer.twitter.com/en/apply-for-access\n",
        "\n",
        "\n",
        "\n",
        "There are several questions you need to answer before they can authorize your request. It seemed daunting at first to have to answer all those questions, but in retrospect I appreciate that Twitter is taking responsibility and ccountability for the data they have- all these among good AI ethics.\n",
        "\n",
        "\n",
        "> Once your developer account is setup, create an app that will make use of the API. There will be a set of keys provided to connect to Twitter's API. SPecifically the:\n",
        "\n",
        "\n",
        "1.   Access token\n",
        "2.   Access token secret\n",
        "3.   API key\n",
        "4.   API secret key\n",
        "\n",
        "\n",
        "Assign variables to each for use in your code as follows:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfnpocFGSdXj"
      },
      "source": [
        "##you get these from twitter authorization for eveloper obtained through the link above.\n",
        "consumer_key = 'wMyunBaUKFTEwjAUPDxTKcwvy'\n",
        "consumer_secret = 'QwglPQurDyLK3Akc5gL7m0GcWa2TxDZmTm9jAqpfE8gwoNrPiv'\n",
        "access_token = '3806985075-w30Rn3LMRHviX0Uy9TfLieSwTg0PW07Eq8qjP29'\n",
        "access_secret = '4AAjV1eUPzOaWOI3jYPTMwjCTCwatATg1tb8kfyOF20er'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GICj6ktPVp8i"
      },
      "source": [
        "####**STEP2:Installing and importing important packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCgVYKLkVzzk",
        "outputId": "4f90a848-56d1-4a7f-8faf-e51ec35567d3"
      },
      "source": [
        "!pip install tweepy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btK7xgh8V_43",
        "outputId": "98063eef-21df-4494-e4ce-97c251daf223"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 241 kB 8.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5Ow2THwWBJn"
      },
      "source": [
        "#import dependencies\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import json\n",
        "from unidecode import unidecode\n",
        "import time\n",
        "import datetime\n",
        "from tqdm import tqdm \n",
        "import pandas as pd  \n",
        "import numpy as np "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLFUOZQ6Wkfa"
      },
      "source": [
        "####Connect to twitter API using the secrets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm1xP78vW6Lw"
      },
      "source": [
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "###wait on rate when set to TRUE allows to continue with search on scipt onec the limit is reached, if FALSE, the script stops running\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJhxaso5YCw6"
      },
      "source": [
        "We then define a function that will take our searh query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzYQjYPyYN7i"
      },
      "source": [
        "def tweetSearch(query, limit):\n",
        "    \"\"\"\n",
        "    This function will search a query provided in the twitter and,\n",
        "    retun a list of all tweets that have a query. \n",
        "    \"\"\"\n",
        "\n",
        "    # Create a blank list variable on which to append the tweets\n",
        "    tweets = []\n",
        "    \n",
        "    # Iterate through Twitter using Tweepy to find our query with our defined limit\n",
        "    for page in tweepy.Cursor(\n",
        "        api.search, q=query, count=limit, tweet_mode=\"extended\"\n",
        "    ).pages(limit):\n",
        "        for tweet in page:\n",
        "            tweets.append(tweet)\n",
        "\n",
        "    # return tweets\n",
        "    return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-weDeh_FYxit"
      },
      "source": [
        "We create another function to save the tweets obtained in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx5bjMVNbbOW"
      },
      "source": [
        "def tweets_to_data_frame(tweets):\n",
        "    \"\"\"\n",
        "    This function will receive tweets and collect specific data from it such as place, tweet's text,likes \n",
        "    retweets and save them into a pandas data frame.\n",
        "    \n",
        "    This function will return a pandas data frame that contains data from twitter.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(data=[tweet.full_text.encode('utf-8') for tweet in tweets], columns=[\"Tweets\"])\n",
        "\n",
        "    df[\"length\"] = np.array([len(tweet.full_text) for tweet in tweets])\n",
        "    df[\"date\"] = np.array([tweet.created_at for tweet in tweets]) #tweepy only takes tweets within the last 7 days\n",
        "    df[\"coordinateS\"] = np.array([tweet.coordinates for tweet in tweets])\n",
        "    df['location'] = np.array([tweet.user.location for tweet in tweets]) #often obtained from the user's twitter bio, remember we need tweets mainly based on Africa\n",
        "    df[\"Language\"] = np.array([tweet.lang for tweet in tweets])\n",
        "    df[\"source\"] = np.array([tweet.source for tweet in tweets])\n",
        "    df[\"likes\"] = np.array([tweet.favorite_count for tweet in tweets])\n",
        "    df[\"retweets\"] = np.array([tweet.retweet_count for tweet in tweets])\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g8syAbab6IH"
      },
      "source": [
        "###Adding twitter hashtags based on Gender Based Violence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmpzP_LbcLEa"
      },
      "source": [
        "Some of those listed below are based on a country's case that spiralled twitter posts. Most were acquired from doing some research on the countries' twitter presence especially with regard to Anti-GBV advocacy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fuOow0Hcpgq"
      },
      "source": [
        "hashtags = [\n",
        "'#GBV',#general\n",
        "'#sexism', #general\n",
        "'#rape', #general\n",
        "'#domesticviolence', #general\n",
        "'#mydressmychoice', #Kenyan based\n",
        "'#ShutItAllDownNamibia', #Namibian based\n",
        "'#TheTotalShutdown', #SA based\n",
        "'#TotalshutdownKE', #Kenyan based\n",
        "'#sayhername', #global\n",
        "'#shutitdown', #SA based\n",
        "'#JusticeForNoura', #Sudan based\n",
        "'#JusticeForOchanya', #Nigerian based\n",
        "'#BringBackOurGirls', #Nigerian based\n",
        "'#metoo'  #global\n",
        "]\n",
        "\n",
        "##you can add more hashtags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fnI2QwZdesN"
      },
      "source": [
        "We run both functions and collect data based on the above hashtags.The following code may take a while to complete so be patient:)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmaDLIWMdm0L"
      },
      "source": [
        "total_tweets = 0\n",
        "\n",
        "\"\"\"\n",
        "The following for loop will collect a tweets that have the hashtags\n",
        " mentioned in the list and save the tweets into csv file\n",
        "\"\"\"\n",
        "\n",
        "for n in tqdm(hashtags):\n",
        "    # first we fetch all tweets that have specific hashtag\n",
        "    hash_tweets = tweetSearch(query=n,limit=7000)\n",
        "    total_tweets += int(len(hash_tweets))\n",
        "    \n",
        "    # second we convert our tweets into datarame\n",
        "    df = tweets_to_data_frame(hash_tweets)\n",
        "    \n",
        "    #third we save the dataframe into csv file\n",
        "    df.to_csv(\"{}_tweets.csv\".format(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaQSrO8kdwlo"
      },
      "source": [
        "# count total number of tweets collected\n",
        "print(\"total_tweets: {}\".format(total_tweets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrWHg2KNdwiQ"
      },
      "source": [
        "df1 = pd.read_csv('#BringBackOurGirls_tweets.csv')\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBbWrDXSdwY7"
      },
      "source": [
        "(df1['location']).value_counts() ###look into the main locations from our dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqZGFD5jkM7C"
      },
      "source": [
        "df1[df1['location']=='Nigeria']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFRDZD1nkM4X"
      },
      "source": [
        "df2 = pd.read_csv('#TotalshutdownKE_tweets.csv')\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM_0WvsqkM1j"
      },
      "source": [
        "(df2['location']).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCVCj2VRkx9m"
      },
      "source": [
        "df3 =pd.read_csv('#GBV_tweets.csv')\n",
        "df3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwsaSgxNkx7b"
      },
      "source": [
        "(df3[df3['location']==str('South Africa')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUg3X74DsYAc"
      },
      "source": [
        "Cleaning up the tweets, here is  sneak peek on how to do that. Note that this notebook was mainly centred around collecting data from tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GemA8T622yB"
      },
      "source": [
        "import re# function to remove special characters\n",
        "def remove_special_characters(text):\n",
        "    # define the pattern to keep\n",
        "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
        "    return re.sub(pat, '', text)\n",
        " \n",
        "# call function\n",
        "remove_special_characters(“007 Not sure@ if this % was #fun! 558923 What do# you think** of it.? $500USD!”)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0NhSW8Xkx33",
        "outputId": "6cc25f18-0627-4533-b93e-7139a75d543b"
      },
      "source": [
        "# remove twitter handles (@user)\n",
        "\n",
        "## remove special characters, numbers, punctuations\n",
        "df3['clean_tweet'] = df3['Tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "#      \n",
        "df3['clean_tweet'] = df3['Tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2])) \n",
        "  \n",
        "df3['clean_tweet']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       b'\"To #EndFGM and #GenderBasedViolence young p...\n",
              "1       b'RT @glocalreporting: Women suffer even more ...\n",
              "2       b\"RT @CedawPT: '40% women have experienced som...\n",
              "3       b'RT @EUinCV: fight gender based violence impo...\n",
              "4       b'To fight gender based violence important hav...\n",
              "                              ...                        \n",
              "2123    b'RT @JulittaOnabanjo: IASC PSEAH champion, US...\n",
              "2124    b'RT @JulittaOnabanjo: IASC PSEAH champion, US...\n",
              "2125    b'RT @salhaj: @Atayeshe @UNFPA \\nFrom medical ...\n",
              "2126    b'RT @salhaj: @Atayeshe @UNFPA \\nFrom medical ...\n",
              "2127    b\"RT @NCADV: The proposed amendments bar intim...\n",
              "Name: clean_tweet, Length: 2128, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3zEuc43xvR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ItDX3UZ3zTT"
      },
      "source": [
        "####Merging the datasets into one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "rnBB_EcJ3wJX",
        "outputId": "2af2ff51-7f0b-4ed3-b20d-b5d2e2041971"
      },
      "source": [
        "df_all = pd.concat([df1,df2,df3])\n",
        "df_all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Tweets</th>\n",
              "      <th>length</th>\n",
              "      <th>date</th>\n",
              "      <th>coordinateS</th>\n",
              "      <th>location</th>\n",
              "      <th>Language</th>\n",
              "      <th>source</th>\n",
              "      <th>likes</th>\n",
              "      <th>retweets</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>b'RT @its_kachi: Id rather BokoHaram take my d...</td>\n",
              "      <td>140</td>\n",
              "      <td>2021-05-24 13:54:27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>b\"RT @chibok276: We've been missing for 2596 d...</td>\n",
              "      <td>67</td>\n",
              "      <td>2021-05-24 13:45:07</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>b'RT @its_kachi: Id rather BokoHaram take my d...</td>\n",
              "      <td>140</td>\n",
              "      <td>2021-05-24 13:12:27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Doing bear stuff Capitola, FL</td>\n",
              "      <td>en</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>b'RT @DolapoAina: #BringBackOurGirls DAY2598\\n...</td>\n",
              "      <td>140</td>\n",
              "      <td>2021-05-24 11:31:37</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lagos, Nigeria</td>\n",
              "      <td>und</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>b'#BringBackOurGirls \\n#HopeEndures \\n#BBOG ht...</td>\n",
              "      <td>63</td>\n",
              "      <td>2021-05-24 10:27:31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>und</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2123</th>\n",
              "      <td>2123</td>\n",
              "      <td>b'RT @JulittaOnabanjo: As IASC PSEAH champion,...</td>\n",
              "      <td>140</td>\n",
              "      <td>2021-05-16 22:26:07</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sweden</td>\n",
              "      <td>en</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>b'RT @JulittaOnabanjo: IASC PSEAH champion, US...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2124</th>\n",
              "      <td>2124</td>\n",
              "      <td>b'RT @JulittaOnabanjo: As IASC PSEAH champion,...</td>\n",
              "      <td>140</td>\n",
              "      <td>2021-05-16 22:23:46</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Instagram @atayesheunfpa</td>\n",
              "      <td>en</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>b'RT @JulittaOnabanjo: IASC PSEAH champion, US...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2125</th>\n",
              "      <td>2125</td>\n",
              "      <td>b'RT @salhaj: Dr @Atayeshe @UNFPA \\nFrom medic...</td>\n",
              "      <td>144</td>\n",
              "      <td>2021-05-16 22:22:48</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Instagram @atayesheunfpa</td>\n",
              "      <td>en</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>b'RT @salhaj: @Atayeshe @UNFPA \\nFrom medical ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2126</th>\n",
              "      <td>2126</td>\n",
              "      <td>b'RT @salhaj: Dr @Atayeshe @UNFPA \\nFrom medic...</td>\n",
              "      <td>144</td>\n",
              "      <td>2021-05-16 22:10:16</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Senegal</td>\n",
              "      <td>en</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>b'RT @salhaj: @Atayeshe @UNFPA \\nFrom medical ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2127</th>\n",
              "      <td>2127</td>\n",
              "      <td>b\"RT @NCADV: The proposed amendments bar intim...</td>\n",
              "      <td>140</td>\n",
              "      <td>2021-05-16 21:37:24</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>en</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>b\"RT @NCADV: The proposed amendments bar intim...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2372 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                        clean_tweet\n",
              "0              0  ...                                                NaN\n",
              "1              1  ...                                                NaN\n",
              "2              2  ...                                                NaN\n",
              "3              3  ...                                                NaN\n",
              "4              4  ...                                                NaN\n",
              "...          ...  ...                                                ...\n",
              "2123        2123  ...  b'RT @JulittaOnabanjo: IASC PSEAH champion, US...\n",
              "2124        2124  ...  b'RT @JulittaOnabanjo: IASC PSEAH champion, US...\n",
              "2125        2125  ...  b'RT @salhaj: @Atayeshe @UNFPA \\nFrom medical ...\n",
              "2126        2126  ...  b'RT @salhaj: @Atayeshe @UNFPA \\nFrom medical ...\n",
              "2127        2127  ...  b\"RT @NCADV: The proposed amendments bar intim...\n",
              "\n",
              "[2372 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQyc8vgfNbcU"
      },
      "source": [
        "Below is another method of collecting live tweets from twitter by specifying the granularity. In my case I chose the country Kenya."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EchO5QyYMHiW",
        "outputId": "6fe7fc81-ed63-4a1b-9fb7-a1997811f885"
      },
      "source": [
        "!pip install twitter"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twitter\n",
            "  Downloading twitter-1.19.3-py2.py3-none-any.whl (50 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 30 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 40 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 3.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: twitter\n",
            "Successfully installed twitter-1.19.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvFwIuMAMKz5",
        "outputId": "2ea462ed-f6b2-4843-8a27-85ba142fc1a7"
      },
      "source": [
        "from twitter import *\n",
        "t = Twitter(auth=OAuth(access_token, access_secret,consumer_key,consumer_secret))\n",
        "result = t.geo.search(query=\"Kenya\", granularity=\"country\")\n",
        "place_id = result['result']['places'][0]['id']\n",
        "\n",
        "result = t.search.tweets(q=\"place:%s\" % place_id)\n",
        "for tweet in result['statuses']:\n",
        "    print (tweet['text'] + \" ~~ \" + tweet['place']['name'] if tweet['place'] else \"Undefined place\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From the streets she rose, to the streets she returned.\n",
            "Dennis Okari dodged a bullet a stunt only seen in movies.\n",
            "M… https://t.co/L107lEzzEn ~~ Kabete\n",
            "@Sphinx_jf Labda umevaa lingerie inakuwinkia ~~ Mombasa\n",
            "@muhammadljunaid Hello am a photographer from nairobi kenya\n",
            "My instagram handle is @simon__kavita https://t.co/mKALaE6p2l ~~ Nairobi\n",
            "@alfu_254 Alifungua prisoners wote juzi 😂😂 ~~ Nairobi\n",
            "@GSaruni @veeeyecee Ssd ~~ Mombasa\n",
            "@Ntobi_ @DKAmbTanzania Viongozi wajihoji na wajiamgalie,Daah Pole kamanda ~~ Mombasa\n",
            "🤣🤣🤣🤣🤣🤣 https://t.co/35F7QyR1cE ~~ Nairobi\n",
            "Ta gúi https://t.co/3TwWN7zVyz ~~ Eldoret\n",
            "Chairman @kipmurkomen , what a low! https://t.co/oUzkcFy7lo ~~ Nairobi\n",
            "@BrunoSigwela This is typically the best art I have come across ~~ Kabete\n",
            "CCTV footage has emerged showing the moment a boda boda rider grabbed a mobile phone from a {police officer} and sp… https://t.co/FOOHe51ocH ~~ Nairobi\n",
            "✦ [ Wash Wash ]\n",
            "\n",
            "Hilarious CCTV Video: Boda Boda Rider Robbing Police Officer\n",
            "\n",
            "https://t.co/97eSuMkpvO ~~ Nairobi\n",
            "Finally tumefikiwa\n",
            "\n",
            "Hi so much https://t.co/FeoEHaCDcY ~~ Kabete\n",
            "@ArnoldBadoo @EduMinKenya This is Meru. Other schools pay between 3000 and 4000 for meals. ~~ Nkubu\n",
            "Everything you hear is unheard of. #airpodspro @ SkyMall Nairobi https://t.co/MCtYG2vDmb ~~ Nairobi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXds_58ropL"
      },
      "source": [
        "Some more resources that show how to acquire data from twitter can be found [here](https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/twitter-data-in-python/) or [here](https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a) or on the [tweepy documentation page](https://docs.tweepy.org/en/latest/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMPcqX0BN_G4"
      },
      "source": [
        "We can ofcourse turn this into a text cleaning task or even find a form of machine learning classification task based on the data collected. This challenge was however mainly based on collecting data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRM-5Eadkxxb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}